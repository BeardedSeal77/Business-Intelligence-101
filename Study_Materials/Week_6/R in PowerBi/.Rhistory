max(people$ages)
people$names[max(people$ages)]
people$names[which.max(people$ages)]
people$names[which.min(people$ages)]
people$group <- ifelse(people$ages < 35, "Young", "Adult")
head(people, 5)
people_sorted <- sort(people, decreasing = FALSE)
people_sorted <- people[order(people$ages, decreasing = FALSE)]
people_sorted <- people[order(people$ages)decreasing = FALSE]
people_sorted <- people[order(people$ages), decreasing = FALSE]
people_sorted <- people[order(people$ages, decreasing = FALSE), ]
people$names[people$group == "Young"]
people$ages[people$group == "Adult"]
table(people$group)
sequence(1, 20, by = 2)
seq(1, 20, by = 2)
rep(100, 5)
v <- c(10, 20, 30, 40, 50)
v
v[2] <- 99
v <- c(10, 20, 30, 40, 50)
v[3] <- 99
v
v[1:3]
v[-2]
summary(ages)
people$ages[people$ages > 40]
gender <- factor(c("M","F","F","M","F","M","F","F"))
people$gender <- gender
table(people$gender)
barplot(table(people$group, main = "Group Counts"))
barplot(table(people$group), main = "Group Counts")
library(arules)
suppressPackageStartupMessages({
library(tidyverse)
library(readr)
library(knitr)
library(naniar)
library(corrplot)
library(recipes)
library(rsample)
library(caret)
library(FSelectorRcpp)
library(glmnet)
library(randomForest)
library(themis)
library(ggplot2)
})
install.packages(naniar)
install.packages("naniar")
suppressPackageStartupMessages({
library(tidyverse)
library(readr)
library(knitr)
library(naniar)
library(corrplot)
library(recipes)
library(rsample)
library(caret)
library(FSelectorRcpp)
library(glmnet)
library(randomForest)
library(themis)
library(ggplot2)
})
install.packages("corrplot")
suppressPackageStartupMessages({
library(tidyverse)
library(readr)
library(knitr)
library(naniar)
library(corrplot)
library(recipes)
library(rsample)
library(caret)
library(FSelectorRcpp)
library(glmnet)
library(randomForest)
library(themis)
library(ggplot2)
})
install.packages("recipes")
suppressPackageStartupMessages({
library(tidyverse)
library(readr)
library(knitr)
library(naniar)
library(corrplot)
library(recipes)
library(rsample)
library(caret)
library(FSelectorRcpp)
library(glmnet)
library(randomForest)
library(themis)
library(ggplot2)
})
install.packages("rsample")
suppressPackageStartupMessages({
library(tidyverse)
library(readr)
library(knitr)
library(naniar)
library(corrplot)
library(recipes)
library(rsample)
library(caret)
library(FSelectorRcpp)
library(glmnet)
library(randomForest)
library(themis)
library(ggplot2)
})
install.packages("caret")
suppressPackageStartupMessages({
library(tidyverse)
library(readr)
library(knitr)
library(naniar)
library(corrplot)
library(recipes)
library(rsample)
library(caret)
library(FSelectorRcpp)
library(glmnet)
library(randomForest)
library(themis)
library(ggplot2)
})
install.packages("FSelectorRcpp")
suppressPackageStartupMessages({
library(tidyverse)
library(readr)
library(knitr)
library(naniar)
library(corrplot)
library(recipes)
library(rsample)
library(caret)
library(FSelectorRcpp)
library(glmnet)
library(randomForest)
library(themis)
library(ggplot2)
})
install.packages("glmnet")
suppressPackageStartupMessages({
library(tidyverse)
library(readr)
library(knitr)
library(naniar)
library(corrplot)
library(recipes)
library(rsample)
library(caret)
library(FSelectorRcpp)
library(glmnet)
library(randomForest)
library(themis)
library(ggplot2)
})
suppressPackageStartupMessages({
library(readr)     # read_csv, write_csv
library(dplyr)     # select, mutate, across, etc.
library(rsample)   # initial_split, training, testing
library(recipes)   # preprocessing pipeline
library(knitr)     # optional: pretty tables in report
})
# Directories
clean_dir <- "02_Project/Data/02_Cleaned"
train_test_dir <- file.path(clean_dir, "prepared")
dir.create(train_test_dir, recursive = TRUE, showWarnings = FALSE)
# List datasets
final_files <- list.files(clean_dir, pattern = "\\.csv$", full.names = TRUE)
final_files
suppressPackageStartupMessages({
library(readr)     # read_csv, write_csv
library(dplyr)     # select, mutate, across, etc.
library(rsample)   # initial_split, training, testing
library(recipes)   # pre-processing pipeline
library(knitr)     # pretty tables in report
})
# Directories
clean_dir <- "02_Project/Data/02_Cleaned"
train_test_dir <- file.path(clean_dir, "prepared")
dir.create(train_test_dir, recursive = TRUE, showWarnings = FALSE)
# List data-sets
final_files <- list.files(clean_dir, pattern = "\\.csv$", full.names = TRUE)
final_files
suppressPackageStartupMessages({
library(readr)     # read_csv, write_csv
library(dplyr)     # select, mutate, across, etc.
library(rsample)   # initial_split, training, testing
library(recipes)   # pre-processing pipeline
library(knitr)     # pretty tables in report
})
# Directories
clean_dir <- "../../Data/02_Cleaned"
train_test_dir <- file.path(clean_dir, "prepared")
dir.create(train_test_dir, recursive = TRUE, showWarnings = FALSE)
# List data-sets
final_files <- list.files(clean_dir, pattern = "\\.csv$", full.names = TRUE)
final_files
suppressPackageStartupMessages({
library(readr)     # read_csv, write_csv
library(dplyr)     # select, mutate, across, etc.
library(rsample)   # initial_split, training, testing
library(recipes)   # pre-processing pipeline
library(knitr)     # pretty tables in report
})
# Directories
clean_dir <- "../../Data/02_Cleaned"
output_dir <- "../../Data/03_Scaled"
# List cleaned data-sets
cleaned_files <- list.files(clean_dir, pattern = "\\.csv$", full.names = TRUE)
cleaned_files
suppressPackageStartupMessages({
library(readr)     # read_csv, write_csv
library(dplyr)     # select, mutate, across, etc.
library(rsample)   # initial_split, training, testing
library(recipes)   # pre-processing pipeline
library(knitr)     # pretty tables in report
})
# Directories
clean_dir <- "../../Data/02_Cleaned"
output_dir <- "../../Data/03_Scaled"
# List cleaned data-sets
cleaned_files <- list.files(clean_dir, pattern = "\\.csv$", full.names = TRUE)
cleaned_files
suppressPackageStartupMessages({
library(readr)     # read_csv, write_csv
library(dplyr)     # select, mutate, across, etc.
library(rsample)   # initial_split, training, testing
library(recipes)   # pre-processing pipeline
library(knitr)     # pretty tables in report
})
# Directories
clean_dir <- "../../Data/02_Cleaned"
output_dir <- "../../Data/03_Scaled"
# List cleaned data-sets
cleaned_files <- list.files(clean_dir, pattern = "\\.csv$", full.names = TRUE)
cleaned_files
suppressPackageStartupMessages({
library(readr)     # read_csv, write_csv
library(dplyr)     # select, mutate, across, etc.
library(rsample)   # initial_split, training, testing
library(recipes)   # pre-processing pipeline
library(knitr)     # pretty tables in report
})
# Directories
clean_dir <- "../../Data/02_Cleaned"
output_dir <- "../../Data/03_Scaled"
# List cleaned data-sets
cleaned_files <- list.files(clean_dir, pattern = "\\.csv$", full.names = TRUE)
cleaned_files
# Initialize list to store processed data-sets
encoded_datasets <- list()
for (file_path in cleaned_files) {
# Load dataset
df <- read_csv(file_path, show_col_types = FALSE)
dataset_name <- tools::file_path_sans_ext(basename(file_path))
# Identify categorical columns
cat_cols <- df %>% select(where(~ is.character(.x) | is.factor(.x))) %>% names()
if (length(cat_cols) > 0) {
for(col in cat_cols){
# Create one-hot encoding for the column
dummy_df <- model.matrix(~ . - 1, data = df[col]) %>% as.data.frame()
# Clean column names: replace spaces, special chars, etc.
names(dummy_df) <- gsub(paste0("^", col), col, names(dummy_df))
names(dummy_df) <- make.names(names(dummy_df))
# Bind dummy columns and remove original column
df <- bind_cols(df, dummy_df)
df[[col]] <- NULL
}
}
# Store processed dataset
encoded_datasets[[dataset_name]] <- df
cat("One-hot encoded all categorical variables with clean names for dataset:", dataset_name, "\n")
}
suppressPackageStartupMessages({
library(readr)     # read_csv, write_csv
library(dplyr)     # select, mutate, across, etc.
library(rsample)   # initial_split, training, testing
library(recipes)   # pre-processing pipeline
library(knitr)     # pretty tables in report
})
# Directories
clean_dir <- "../../Data/02_Cleaned"
output_dir <- "../../Data/03_Scaled"
# List cleaned data-sets
cleaned_files <- list.files(clean_dir, pattern = "\\.csv$", full.names = TRUE)
cleaned_files
# Initialize list to store processed data-sets
encoded_datasets <- list()
for (file_path in cleaned_files) {
# Load dataset
df <- read_csv(file_path, show_col_types = FALSE)
dataset_name <- tools::file_path_sans_ext(basename(file_path))
# Identify categorical columns
cat_cols <- df %>% select(where(~ is.character(.x) | is.factor(.x))) %>% names()
if (length(cat_cols) > 0) {
for(col in cat_cols){
n_levels <- n_distinct(df[[col]])
if(n_levels == 1){
# Drop single-level columns
df[[col]] <- NULL
next
}
# One-hot encoding for columns with 2+ levels
dummy_df <- model.matrix(~ . - 1, data = df[col]) %>% as.data.frame()
# Clean column names
names(dummy_df) <- gsub(paste0("^", col), col, names(dummy_df))
names(dummy_df) <- make.names(names(dummy_df))
# Bind dummy columns and remove original column
df <- bind_cols(df, dummy_df)
df[[col]] <- NULL
}
}
# Store processed dataset
encoded_datasets[[dataset_name]] <- df
cat("One-hot encoded categorical variables (dropped single-level) for dataset:", dataset_name, "\n")
}
head(encoded_datasets[["access-to-health-care_national_zaf_final"]])
suppressPackageStartupMessages({
library(readr)     # read_csv, write_csv
library(dplyr)     # select, mutate, across, etc.
library(rsample)   # initial_split, training, testing
library(recipes)   # pre-processing pipeline
library(knitr)     # pretty tables in report
})
# Directories
clean_dir <- "../../Data/02_Cleaned"
output_dir <- "../../Data/03_Scaled"
# List cleaned data-sets
cleaned_files <- list.files(clean_dir, pattern = "\\.csv$", full.names = TRUE)
cleaned_files
for (file_path in cleaned_files) {
dataset_name <- tools::file_path_sans_ext(basename(file_path))
cat("Dataset:", dataset_name, "\n")
# Load dataset
df <- read_csv(file_path, show_col_types = FALSE)
# Create table of column names and data types
col_info <- data.frame(
Column = names(df),
DataType = sapply(df, class),
stringsAsFactors = FALSE
)
print(col_info)
cat("\n----------------------------------------\n\n")
}
View(col_info)
View(col_info)
suppressPackageStartupMessages({
library(readr)     # read_csv, write_csv
library(dplyr)     # select, mutate, across, etc.
library(rsample)   # initial_split, training, testing
library(recipes)   # pre-processing pipeline
library(knitr)     # pretty tables in report
})
# Directories
clean_dir <- "../../Data/02_Cleaned"
output_dir <- "../../Data/03_Scaled"
# List cleaned data-sets
cleaned_files <- list.files(clean_dir, pattern = "\\.csv$", full.names = TRUE)
cleaned_files
for (file_path in cleaned_files) {
dataset_name <- tools::file_path_sans_ext(basename(file_path))
cat("Dataset:", dataset_name, "\n")
# Load dataset
df <- read_csv(file_path, show_col_types = FALSE)
# Create table of column names and data types
col_info <- data.frame(
Column = names(df),
DataType = sapply(df, class),
stringsAsFactors = FALSE
)
print(col_info)
cat("\n----------------------------------------\n\n")
}
View(col_info)
View(col_info)
suppressPackageStartupMessages({
library(readr)     # read_csv, write_csv
library(dplyr)     # select, mutate, across, etc.
library(rsample)   # initial_split, training, testing
library(recipes)   # pre-processing pipeline
library(knitr)     # pretty tables in report
})
# Directories
clean_dir <- "../../Data/02_Cleaned"
output_dir <- "../../Data/03_Scaled"
# List cleaned data-sets
cleaned_files <- list.files(clean_dir, pattern = "\\.csv$", full.names = TRUE)
cleaned_files
# 1. Load the dataset
data(iris)
df <- iris[, 1:2]  # Use Sepal.Length and Sepal.Width for 2D plotting
# 2. Plot raw data
plot(df$Sepal.Length, df$Sepal.Width,
main = "Raw Iris Data",
xlab = "Sepal Length",
ylab = "Sepal Width",
col = "gray",
pch = 19)
# 3. Apply K-Means clustering
set.seed(42)
kmeans_result <- kmeans(df, centers = 3)
points(kmeans_result$centers, col = 1:3, pch = 8, cex = 2)  # Centroids
# ----------------------------
# K-Means Clustering Example
# ----------------------------
# 1. Load the built-in iris dataset
data(iris)
# Select only the first two columns for 2D plotting: Sepal.Length and Sepal.Width
df <- iris[, 1:2]
# ----------------------------
# 2. Plot the raw data
# ----------------------------
plot(df$Sepal.Length, df$Sepal.Width,
main = "Raw Iris Data",
xlab = "Sepal Length",
ylab = "Sepal Width",
col = "gray",    # All points in gray for raw view
pch = 19)        # Solid circle points
# ----------------------------
# 3. Apply K-Means clustering
# ----------------------------
set.seed(42)             # For reproducibility
kmeans_result <- kmeans(df, centers = 3)  # Cluster into 3 groups
# kmeans_result$cluster contains the cluster assignment for each row
# ----------------------------
# 4. Define colors for clusters
# ----------------------------
cluster_colors <- c("red", "green", "blue")           # Assign a color for each cluster
point_colors <- cluster_colors[kmeans_result$cluster] # Map cluster number to color
# ----------------------------
# 5. Plot the clustered data
# ----------------------------
plot(df$Sepal.Length, df$Sepal.Width,
col = point_colors,    # Color points based on cluster
pch = 19,              # Solid circle points
xlab = "Sepal Length",
ylab = "Sepal Width",
main = "K-Means Clustering with Colored Clusters")
# Add cluster centroids to the plot
points(kmeans_result$centers,
col = cluster_colors,  # Color centroids same as cluster
pch = 8,               # Star-like shape for centroids
cex = 2)               # Make centroids larger
# Add a legend for clarity
legend("topright", legend = paste("Cluster", 1:3),
col = cluster_colors, pch = 19)
# kmeans_result$cluster contains the cluster assignment for each row
kmeans_result$cluster
# ----------------------------
# K-Nearest Neighbors (KNN) Classification Example
# ----------------------------
# 1. Load required library
library(class)  # Provides the knn() function
# 2. Load the built-in iris dataset
data(iris)
# Select features (Sepal.Length and Sepal.Width for 2D plotting)
X <- iris[, 1:2]
# Labels: Species
y <- iris$Species
# ----------------------------
# 3. Split data into training and test sets
# ----------------------------
set.seed(42)  # For reproducibility
# Randomly sample 70% of the rows for training
train_idx <- sample(1:nrow(X), 0.7 * nrow(X))
X_train <- X[train_idx, ]
X_test  <- X[-train_idx, ]
y_train <- y[train_idx]
y_test  <- y[-train_idx]
# ----------------------------
# 4. Train KNN classifier
# ----------------------------
k <- 5  # Number of neighbors
# knn() returns the predicted class labels for the test set
y_pred <- knn(train = X_train, test = X_test, cl = y_train, k = k)
# ----------------------------
# 5. Evaluate accuracy
# ----------------------------
accuracy <- sum(y_pred == y_test) / length(y_test)
print(paste("Test Accuracy:", round(accuracy, 3)))
# ----------------------------
# 6. Plot KNN results
# ----------------------------
# Assign numeric colors to each predicted class
class_colors <- as.numeric(y_pred)
plot(X_test$Sepal.Length, X_test$Sepal.Width,
col = class_colors,  # Color points based on predicted class
pch = 19,            # Solid circle points
xlab = "Sepal Length",
ylab = "Sepal Width",
main = paste("KNN Classification (k =", k, ")"))
# Add a legend for clarity
legend("topright", legend = levels(y), col = 1:3, pch = 19)
data("mpg"
data("mpg")
data("mpg")
library(sp)
install.packages("rworldmap")
library(rworldmap)
data("mpg")
library(ggplot2)
data("mpg")
head(mpg)
setwd("C:/Users/juano/Desktop/Belgium/Third Year/BIN381/Week 6/PowerBi R")
# Save dataset as CSV file
write.csv(mpg, "mpg_data.csv", row.names = FALSE)
